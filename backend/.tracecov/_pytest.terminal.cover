       # mypy: allow-untyped-defs
>>>>>> """Terminal reporting of the full testing process.
       
       This is a good source for looking at the various reporting hooks.
       """
       
>>>>>> from __future__ import annotations
       
>>>>>> import argparse
>>>>>> from collections import Counter
>>>>>> from collections.abc import Callable
>>>>>> from collections.abc import Generator
>>>>>> from collections.abc import Mapping
>>>>>> from collections.abc import Sequence
>>>>>> import dataclasses
>>>>>> import datetime
>>>>>> from functools import partial
>>>>>> import inspect
>>>>>> from pathlib import Path
>>>>>> import platform
>>>>>> import sys
>>>>>> import textwrap
>>>>>> from typing import Any
>>>>>> from typing import ClassVar
>>>>>> from typing import final
>>>>>> from typing import Literal
>>>>>> from typing import NamedTuple
>>>>>> from typing import TextIO
>>>>>> from typing import TYPE_CHECKING
>>>>>> import warnings
       
>>>>>> import pluggy
       
>>>>>> from _pytest import compat
>>>>>> from _pytest import nodes
>>>>>> from _pytest import timing
>>>>>> from _pytest._code import ExceptionInfo
>>>>>> from _pytest._code.code import ExceptionRepr
>>>>>> from _pytest._io import TerminalWriter
>>>>>> from _pytest._io.wcwidth import wcswidth
>>>>>> import _pytest._version
>>>>>> from _pytest.compat import running_on_ci
>>>>>> from _pytest.config import _PluggyPlugin
>>>>>> from _pytest.config import Config
>>>>>> from _pytest.config import ExitCode
>>>>>> from _pytest.config import hookimpl
>>>>>> from _pytest.config.argparsing import Parser
>>>>>> from _pytest.nodes import Item
>>>>>> from _pytest.nodes import Node
>>>>>> from _pytest.pathlib import absolutepath
>>>>>> from _pytest.pathlib import bestrelpath
>>>>>> from _pytest.reports import BaseReport
>>>>>> from _pytest.reports import CollectReport
>>>>>> from _pytest.reports import TestReport
       
       
>>>>>> if TYPE_CHECKING:
>>>>>>     from _pytest.main import Session
       
       
>>>>>> REPORT_COLLECTING_RESOLUTION = 0.5
       
>>>>>> KNOWN_TYPES = (
           "failed",
           "passed",
           "skipped",
           "deselected",
           "xfailed",
           "xpassed",
           "warnings",
           "error",
           "subtests passed",
           "subtests failed",
           "subtests skipped",
       )
       
>>>>>> _REPORTCHARS_DEFAULT = "fE"
       
       
>>>>>> class MoreQuietAction(argparse.Action):
           """A modified copy of the argparse count action which counts down and updates
           the legacy quiet attribute at the same time.
       
           Used to unify verbosity handling.
           """
       
>>>>>>     def __init__(
               self,
>>>>>>         option_strings: Sequence[str],
>>>>>>         dest: str,
>>>>>>         default: object = None,
>>>>>>         required: bool = False,
>>>>>>         help: str | None = None,
>>>>>>     ) -> None:
    2:         super().__init__(
    1:             option_strings=option_strings,
    1:             dest=dest,
    1:             nargs=0,
    1:             default=default,
    1:             required=required,
    1:             help=help,
               )
       
>>>>>>     def __call__(
               self,
>>>>>>         parser: argparse.ArgumentParser,
>>>>>>         namespace: argparse.Namespace,
>>>>>>         values: str | Sequence[object] | None,
>>>>>>         option_string: str | None = None,
>>>>>>     ) -> None:
    4:         new_count = getattr(namespace, self.dest, 0) - 1
    4:         setattr(namespace, self.dest, new_count)
               # todo Deprecate config.quiet
    4:         namespace.quiet = getattr(namespace, "quiet", 0) + 1
       
       
>>>>>> class TestShortLogReport(NamedTuple):
           """Used to store the test status result category, shortletter and verbose word.
           For example ``"rerun", "R", ("RERUN", {"yellow": True})``.
       
           :ivar category:
               The class of result, for example ``“passed”``, ``“skipped”``, ``“error”``, or the empty string.
       
           :ivar letter:
               The short letter shown as testing progresses, for example ``"."``, ``"s"``, ``"E"``, or the empty string.
       
           :ivar word:
               Verbose word is shown as testing progresses in verbose mode, for example ``"PASSED"``, ``"SKIPPED"``,
               ``"ERROR"``, or the empty string.
           """
       
>>>>>>     category: str
>>>>>>     letter: str
>>>>>>     word: str | tuple[str, Mapping[str, bool]]
       
       
>>>>>> def pytest_addoption(parser: Parser) -> None:
    1:     group = parser.getgroup("terminal reporting", "Reporting", after="general")
    2:     group._addoption(  # private to use reserved lower-case short option
    1:         "-v",
    1:         "--verbose",
    1:         action="count",
    1:         default=0,
    1:         dest="verbose",
    1:         help="Increase verbosity",
           )
    2:     group.addoption(
    1:         "--no-header",
    1:         action="store_true",
    1:         default=False,
    1:         dest="no_header",
    1:         help="Disable header",
           )
    2:     group.addoption(
    1:         "--no-summary",
    1:         action="store_true",
    1:         default=False,
    1:         dest="no_summary",
    1:         help="Disable summary",
           )
    2:     group.addoption(
    1:         "--no-fold-skipped",
    1:         action="store_false",
    1:         dest="fold_skipped",
    1:         default=True,
    1:         help="Do not fold skipped tests in short summary.",
           )
    2:     group.addoption(
    1:         "--force-short-summary",
    1:         action="store_true",
    1:         dest="force_short_summary",
    1:         default=False,
    1:         help="Force condensed summary output regardless of verbosity level.",
           )
    2:     group._addoption(  # private to use reserved lower-case short option
    1:         "-q",
    1:         "--quiet",
    1:         action=MoreQuietAction,
    1:         default=0,
    1:         dest="verbose",
    1:         help="Decrease verbosity",
           )
    2:     group.addoption(
    1:         "--verbosity",
    1:         dest="verbose",
    1:         type=int,
    1:         default=0,
    1:         help="Set verbosity. Default: 0.",
           )
    2:     group._addoption(  # private to use reserved lower-case short option
    1:         "-r",
    1:         action="store",
    1:         dest="reportchars",
    1:         default=_REPORTCHARS_DEFAULT,
    1:         metavar="chars",
    1:         help="Show extra test summary info as specified by chars: (f)ailed, "
               "(E)rror, (s)kipped, (x)failed, (X)passed, "
               "(p)assed, (P)assed with output, (a)ll except passed (p/P), or (A)ll. "
               "(w)arnings are enabled by default (see --disable-warnings), "
               "'N' can be used to reset the list. (default: 'fE').",
           )
    2:     group.addoption(
    1:         "--disable-warnings",
    1:         "--disable-pytest-warnings",
    1:         default=False,
    1:         dest="disable_warnings",
    1:         action="store_true",
    1:         help="Disable warnings summary",
           )
    2:     group._addoption(  # private to use reserved lower-case short option
    1:         "-l",
    1:         "--showlocals",
    1:         action="store_true",
    1:         dest="showlocals",
    1:         default=False,
    1:         help="Show locals in tracebacks (disabled by default)",
           )
    2:     group.addoption(
    1:         "--no-showlocals",
    1:         action="store_false",
    1:         dest="showlocals",
    1:         help="Hide locals in tracebacks (negate --showlocals passed through addopts)",
           )
    2:     group.addoption(
    1:         "--tb",
    1:         metavar="style",
    1:         action="store",
    1:         dest="tbstyle",
    1:         default="auto",
    1:         choices=["auto", "long", "short", "no", "line", "native"],
    1:         help="Traceback print mode (auto/long/short/line/native/no)",
           )
    2:     group.addoption(
    1:         "--xfail-tb",
    1:         action="store_true",
    1:         dest="xfail_tb",
    1:         default=False,
    1:         help="Show tracebacks for xfail (as long as --tb != no)",
           )
    2:     group.addoption(
    1:         "--show-capture",
    1:         action="store",
    1:         dest="showcapture",
    1:         choices=["no", "stdout", "stderr", "log", "all"],
    1:         default="all",
    1:         help="Controls how captured stdout/stderr/log is shown on failed tests. "
               "Default: all.",
           )
    2:     group.addoption(
    1:         "--fulltrace",
    1:         "--full-trace",
    1:         action="store_true",
    1:         default=False,
    1:         help="Don't cut any tracebacks (default is to cut)",
           )
    2:     group.addoption(
    1:         "--color",
    1:         metavar="color",
    1:         action="store",
    1:         dest="color",
    1:         default="auto",
    1:         choices=["yes", "no", "auto"],
    1:         help="Color terminal output (yes/no/auto)",
           )
    2:     group.addoption(
    1:         "--code-highlight",
    1:         default="yes",
    1:         choices=["yes", "no"],
    1:         help="Whether code should be highlighted (only if --color is also enabled). "
               "Default: yes.",
           )
       
    2:     parser.addini(
    1:         "console_output_style",
    1:         help='Console output: "classic", or with additional progress information '
               '("progress" (percentage) | "count" | "progress-even-when-capture-no" (forces '
               "progress even when capture=no)",
    1:         default="progress",
           )
    2:     Config._add_verbosity_ini(
    1:         parser,
    1:         Config.VERBOSITY_TEST_CASES,
               help=(
    1:             "Specify a verbosity level for test case execution, overriding the main level. "
                   "Higher levels will provide more detailed information about each test case executed."
               ),
           )
       
       
>>>>>> def pytest_configure(config: Config) -> None:
    1:     reporter = TerminalReporter(config, sys.stdout)
    1:     config.pluginmanager.register(reporter, "terminalreporter")
    1:     if config.option.debug or config.option.traceconfig:
       
>>>>>>         def mywriter(tags, args):
>>>>>>             msg = " ".join(map(str, args))
>>>>>>             reporter.write_line("[traceconfig] " + msg)
       
>>>>>>         config.trace.root.setprocessor("pytest:config", mywriter)
       
           # See terminalprogress.py.
           # On Windows it's safe to load by default.
    1:     if sys.platform == "win32":
>>>>>>         config.pluginmanager.import_plugin("terminalprogress")
       
       
>>>>>> def getreportopt(config: Config) -> str:
    1:     reportchars: str = config.option.reportchars
       
    1:     old_aliases = {"F", "S"}
    1:     reportopts = ""
    3:     for char in reportchars:
    2:         if char in old_aliases:
>>>>>>             char = char.lower()
    2:         if char == "a":
>>>>>>             reportopts = "sxXEf"
    2:         elif char == "A":
>>>>>>             reportopts = "PpsxXEf"
    2:         elif char == "N":
>>>>>>             reportopts = ""
    2:         elif char not in reportopts:
    2:             reportopts += char
       
    1:     if not config.option.disable_warnings and "w" not in reportopts:
    1:         reportopts = "w" + reportopts
>>>>>>     elif config.option.disable_warnings and "w" in reportopts:
>>>>>>         reportopts = reportopts.replace("w", "")
       
    1:     return reportopts
       
       
>>>>>> @hookimpl(trylast=True)  # after _pytest.runner
>>>>>> def pytest_report_teststatus(report: BaseReport) -> tuple[str, str, str]:
   89:     letter = "F"
   89:     if report.passed:
   89:         letter = "."
>>>>>>     elif report.skipped:
>>>>>>         letter = "s"
       
   89:     outcome: str = report.outcome
   89:     if report.when in ("collect", "setup", "teardown") and outcome == "failed":
>>>>>>         outcome = "error"
>>>>>>         letter = "E"
       
   89:     return outcome, letter, outcome.upper()
       
       
>>>>>> @dataclasses.dataclass
>>>>>> class WarningReport:
           """Simple structure to hold warnings information captured by ``pytest_warning_recorded``.
       
           :ivar str message:
               User friendly message about the warning.
           :ivar str|None nodeid:
               nodeid that generated the warning (see ``get_location``).
           :ivar tuple fslocation:
               File system location of the source of the warning (see ``get_location``).
           """
       
>>>>>>     message: str
>>>>>>     nodeid: str | None = None
>>>>>>     fslocation: tuple[str, int] | None = None
       
>>>>>>     count_towards_summary: ClassVar = True
       
>>>>>>     def get_location(self, config: Config) -> str | None:
               """Return the more user-friendly information about the location of a warning, or None."""
    7:         if self.nodeid:
>>>>>>             return self.nodeid
    7:         if self.fslocation:
    7:             filename, linenum = self.fslocation
    7:             relpath = bestrelpath(config.invocation_params.dir, absolutepath(filename))
    7:             return f"{relpath}:{linenum}"
>>>>>>         return None
       
       
>>>>>> @final
>>>>>> class TerminalReporter:
>>>>>>     def __init__(self, config: Config, file: TextIO | None = None) -> None:
    1:         import _pytest.config
       
    1:         self.config = config
    1:         self._numcollected = 0
    1:         self._session: Session | None = None
    1:         self._showfspath: bool | None = None
       
    1:         self.stats: dict[str, list[Any]] = {}
    1:         self._main_color: str | None = None
    1:         self._known_types: list[str] | None = None
    1:         self.startpath = config.invocation_params.dir
    1:         if file is None:
>>>>>>             file = sys.stdout
    1:         self._tw = _pytest.config.create_terminal_writer(config, file)
    1:         self._screen_width = self._tw.fullwidth
    1:         self.currentfspath: None | Path | str | int = None
    1:         self.reportchars = getreportopt(config)
    1:         self.foldskipped = config.option.fold_skipped
    1:         self.hasmarkup = self._tw.hasmarkup
               # isatty should be a method but was wrongly implemented as a boolean.
               # We use CallableBool here to support both.
    1:         self.isatty = compat.CallableBool(file.isatty())
    1:         self._progress_nodeids_reported: set[str] = set()
    1:         self._timing_nodeids_reported: set[str] = set()
    1:         self._show_progress_info = self._determine_show_progress_info()
    1:         self._collect_report_last_write = timing.Instant()
    1:         self._already_displayed_warnings: int | None = None
    1:         self._keyboardinterrupt_memo: ExceptionRepr | None = None
       
>>>>>>     def _determine_show_progress_info(
               self,
>>>>>>     ) -> Literal["progress", "count", "times", False]:
               """Return whether we should display progress information based on the current config."""
               # do not show progress if we are not capturing output (#3038) unless explicitly
               # overridden by progress-even-when-capture-no
               if (
    1:             self.config.getoption("capture", "no") == "no"
>>>>>>             and self.config.getini("console_output_style")
>>>>>>             != "progress-even-when-capture-no"
               ):
>>>>>>             return False
               # do not show progress if we are showing fixture setup/teardown
    1:         if self.config.getoption("setupshow", False):
>>>>>>             return False
    1:         cfg: str = self.config.getini("console_output_style")
    1:         if cfg in {"progress", "progress-even-when-capture-no"}:
    1:             return "progress"
>>>>>>         elif cfg == "count":
>>>>>>             return "count"
>>>>>>         elif cfg == "times":
>>>>>>             return "times"
               else:
>>>>>>             return False
       
>>>>>>     @property
>>>>>>     def verbosity(self) -> int:
    4:         verbosity: int = self.config.option.verbose
    4:         return verbosity
       
>>>>>>     @property
>>>>>>     def showheader(self) -> bool:
    1:         return self.verbosity >= 0
       
>>>>>>     @property
>>>>>>     def no_header(self) -> bool:
    1:         return bool(self.config.option.no_header)
       
>>>>>>     @property
>>>>>>     def no_summary(self) -> bool:
    1:         return bool(self.config.option.no_summary)
       
>>>>>>     @property
>>>>>>     def showfspath(self) -> bool:
   89:         if self._showfspath is None:
   89:             return self.config.get_verbosity(Config.VERBOSITY_TEST_CASES) >= 0
>>>>>>         return self._showfspath
       
>>>>>>     @showfspath.setter
>>>>>>     def showfspath(self, value: bool | None) -> None:
>>>>>>         self._showfspath = value
       
>>>>>>     @property
>>>>>>     def showlongtestinfo(self) -> bool:
   89:         return self.config.get_verbosity(Config.VERBOSITY_TEST_CASES) > 0
       
>>>>>>     @property
>>>>>>     def reported_progress(self) -> int:
               """The amount of items reported in the progress so far.
       
               :meta private:
               """
  101:         return len(self._progress_nodeids_reported)
       
>>>>>>     def hasopt(self, char: str) -> bool:
    4:         char = {"xfailed": "x", "skipped": "s"}.get(char, char)
    4:         return char in self.reportchars
       
>>>>>>     def write_fspath_result(self, nodeid: str, res: str, **markup: bool) -> None:
   89:         fspath = self.config.rootpath / nodeid.split("::")[0]
   89:         if self.currentfspath is None or fspath != self.currentfspath:
    5:             if self.currentfspath is not None and self._show_progress_info:
    4:                 self._write_progress_information_filling_space()
    5:             self.currentfspath = fspath
    5:             relfspath = bestrelpath(self.startpath, fspath)
    5:             self._tw.line()
    5:             self._tw.write(relfspath + " ")
   89:         self._tw.write(res, flush=True, **markup)
       
>>>>>>     def write_ensure_prefix(self, prefix: str, extra: str = "", **kwargs) -> None:
>>>>>>         if self.currentfspath != prefix:
>>>>>>             self._tw.line()
>>>>>>             self.currentfspath = prefix
>>>>>>             self._tw.write(prefix)
>>>>>>         if extra:
>>>>>>             self._tw.write(extra, **kwargs)
>>>>>>             self.currentfspath = -2
       
>>>>>>     def ensure_newline(self) -> None:
   10:         if self.currentfspath:
    1:             self._tw.line()
    1:             self.currentfspath = None
       
>>>>>>     def wrap_write(
               self,
>>>>>>         content: str,
               *,
>>>>>>         flush: bool = False,
>>>>>>         margin: int = 8,
>>>>>>         line_sep: str = "\n",
>>>>>>         **markup: bool,
>>>>>>     ) -> None:
               """Wrap message with margin for progress info."""
>>>>>>         width_of_current_line = self._tw.width_of_current_line
>>>>>>         wrapped = line_sep.join(
>>>>>>             textwrap.wrap(
>>>>>>                 " " * width_of_current_line + content,
>>>>>>                 width=self._screen_width - margin,
>>>>>>                 drop_whitespace=True,
>>>>>>                 replace_whitespace=False,
                   ),
               )
>>>>>>         wrapped = wrapped[width_of_current_line:]
>>>>>>         self._tw.write(wrapped, flush=flush, **markup)
       
>>>>>>     def write(self, content: str, *, flush: bool = False, **markup: bool) -> None:
    5:         self._tw.write(content, flush=flush, **markup)
       
>>>>>>     def write_raw(self, content: str, *, flush: bool = False) -> None:
>>>>>>         self._tw.write_raw(content, flush=flush)
       
>>>>>>     def flush(self) -> None:
  178:         self._tw.flush()
       
>>>>>>     def write_line(self, line: str | bytes, **markup: bool) -> None:
    7:         if not isinstance(line, str):
>>>>>>             line = str(line, errors="replace")
    7:         self.ensure_newline()
    7:         self._tw.line(line, **markup)
       
>>>>>>     def rewrite(self, line: str, **markup: bool) -> None:
               """Rewinds the terminal cursor to the beginning and writes the given line.
       
               :param erase:
                   If True, will also add spaces until the full terminal width to ensure
                   previous lines are properly erased.
       
               The rest of the keyword arguments are markup instructions.
               """
>>>>>>         erase = markup.pop("erase", False)
>>>>>>         if erase:
>>>>>>             fill_count = self._tw.fullwidth - len(line) - 1
>>>>>>             fill = " " * fill_count
               else:
>>>>>>             fill = ""
>>>>>>         line = str(line)
>>>>>>         self._tw.write("\r" + line + fill, **markup)
       
>>>>>>     def write_sep(
               self,
>>>>>>         sep: str,
>>>>>>         title: str | None = None,
>>>>>>         fullwidth: int | None = None,
>>>>>>         **markup: bool,
>>>>>>     ) -> None:
    3:         self.ensure_newline()
    3:         self._tw.sep(sep, title, fullwidth, **markup)
       
>>>>>>     def section(self, title: str, sep: str = "=", **kw: bool) -> None:
>>>>>>         self._tw.sep(sep, title, **kw)
       
>>>>>>     def line(self, msg: str, **kw: bool) -> None:
>>>>>>         self._tw.line(msg, **kw)
       
>>>>>>     def _add_stats(self, category: str, items: Sequence[Any]) -> None:
  274:         set_main_color = category not in self.stats
  274:         self.stats.setdefault(category, []).extend(items)
  274:         if set_main_color:
    3:             self._set_main_color()
       
>>>>>>     def pytest_internalerror(self, excrepr: ExceptionRepr) -> bool:
>>>>>>         for line in str(excrepr).split("\n"):
>>>>>>             self.write_line("INTERNALERROR> " + line)
>>>>>>         return True
       
>>>>>>     def pytest_warning_recorded(
               self,
>>>>>>         warning_message: warnings.WarningMessage,
>>>>>>         nodeid: str,
>>>>>>     ) -> None:
    7:         from _pytest.warnings import warning_record_to_str
       
    7:         fslocation = warning_message.filename, warning_message.lineno
    7:         message = warning_record_to_str(warning_message)
       
   14:         warning_report = WarningReport(
    7:             fslocation=fslocation, message=message, nodeid=nodeid
               )
    7:         self._add_stats("warnings", [warning_report])
       
>>>>>>     def pytest_plugin_registered(self, plugin: _PluggyPlugin) -> None:
   42:         if self.config.option.traceconfig:
>>>>>>             msg = f"PLUGIN registered: {plugin}"
                   # XXX This event may happen during setup/teardown time
                   #     which unfortunately captures our output here
                   #     which garbles our output if we use self.write_line.
>>>>>>             self.write_line(msg)
       
>>>>>>     def pytest_deselected(self, items: Sequence[Item]) -> None:
>>>>>>         self._add_stats("deselected", items)
       
>>>>>>     def pytest_runtest_logstart(
>>>>>>         self, nodeid: str, location: tuple[str, int | None, str]
>>>>>>     ) -> None:
   89:         fspath, lineno, domain = location
               # Ensure that the path is printed before the
               # 1st test of a module starts running.
   89:         if self.showlongtestinfo:
>>>>>>             line = self._locationline(nodeid, fspath, lineno, domain)
>>>>>>             self.write_ensure_prefix(line, "")
>>>>>>             self.flush()
   89:         elif self.showfspath:
   89:             self.write_fspath_result(nodeid, "")
   89:             self.flush()
       
>>>>>>     def pytest_runtest_logreport(self, report: TestReport) -> None:
  267:         self._tests_ran = True
  267:         rep = report
       
  534:         res = TestShortLogReport(
  267:             *self.config.hook.pytest_report_teststatus(report=rep, config=self.config)
               )
  267:         category, letter, word = res.category, res.letter, res.word
  267:         if not isinstance(word, tuple):
  267:             markup = None
               else:
>>>>>>             word, markup = word
  267:         self._add_stats(category, [rep])
  267:         if not letter and not word:
                   # Probably passed setup/teardown.
  178:             return
   89:         if markup is None:
   89:             was_xfail = hasattr(report, "wasxfail")
   89:             if rep.passed and not was_xfail:
   89:                 markup = {"green": True}
>>>>>>             elif rep.passed and was_xfail:
>>>>>>                 markup = {"yellow": True}
>>>>>>             elif rep.failed:
>>>>>>                 markup = {"red": True}
>>>>>>             elif rep.skipped:
>>>>>>                 markup = {"yellow": True}
                   else:
>>>>>>                 markup = {}
   89:         self._progress_nodeids_reported.add(rep.nodeid)
   89:         if self.config.get_verbosity(Config.VERBOSITY_TEST_CASES) <= 0:
   89:             self._tw.write(letter, **markup)
                   # When running in xdist, the logreport and logfinish of multiple
                   # items are interspersed, e.g. `logreport`, `logreport`,
                   # `logfinish`, `logfinish`. To avoid the "past edge" calculation
                   # from getting confused and overflowing (#7166), do the past edge
                   # printing here and not in logfinish, except for the 100% which
                   # should only be printed after all teardowns are finished.
   89:             if self._show_progress_info and not self._is_last_item:
   88:                 self._write_progress_information_if_past_edge()
               else:
>>>>>>             line = self._locationline(rep.nodeid, *rep.location)
>>>>>>             running_xdist = hasattr(rep, "node")
>>>>>>             if not running_xdist:
>>>>>>                 self.write_ensure_prefix(line, word, **markup)
>>>>>>                 if rep.skipped or hasattr(report, "wasxfail"):
>>>>>>                     reason = _get_raw_skip_reason(rep)
>>>>>>                     if self.config.get_verbosity(Config.VERBOSITY_TEST_CASES) < 2:
>>>>>>                         available_width = (
>>>>>>                             (self._tw.fullwidth - self._tw.width_of_current_line)
>>>>>>                             - len(" [100%]")
>>>>>>                             - 1
                               )
>>>>>>                         formatted_reason = _format_trimmed(
>>>>>>                             " ({})", reason, available_width
                               )
                           else:
>>>>>>                         formatted_reason = f" ({reason})"
       
>>>>>>                     if reason and formatted_reason is not None:
>>>>>>                         self.wrap_write(formatted_reason)
>>>>>>                 if self._show_progress_info:
>>>>>>                     self._write_progress_information_filling_space()
                   else:
>>>>>>                 self.ensure_newline()
>>>>>>                 self._tw.write(f"[{rep.node.gateway.id}]")
>>>>>>                 if self._show_progress_info:
>>>>>>                     self._tw.write(
>>>>>>                         self._get_progress_information_message() + " ", cyan=True
                           )
                       else:
>>>>>>                     self._tw.write(" ")
>>>>>>                 self._tw.write(word, **markup)
>>>>>>                 self._tw.write(" " + line)
>>>>>>                 self.currentfspath = -2
   89:         self.flush()
       
>>>>>>     @property
>>>>>>     def _is_last_item(self) -> bool:
   95:         assert self._session is not None
   95:         return self.reported_progress == self._session.testscollected
       
>>>>>>     @hookimpl(wrapper=True)
>>>>>>     def pytest_runtestloop(self) -> Generator[None, object, object]:
    1:         result = yield
       
               # Write the final/100% progress -- deferred until the loop is complete.
               if (
    1:             self.config.get_verbosity(Config.VERBOSITY_TEST_CASES) <= 0
    1:             and self._show_progress_info
    1:             and self.reported_progress
               ):
    1:             self._write_progress_information_filling_space()
       
    1:         return result
       
>>>>>>     def _get_progress_information_message(self) -> str:
    5:         assert self._session
    5:         collected = self._session.testscollected
    5:         if self._show_progress_info == "count":
>>>>>>             if collected:
>>>>>>                 progress = self.reported_progress
>>>>>>                 counter_format = f"{{:{len(str(collected))}d}}"
>>>>>>                 format_string = f" [{counter_format}/{{}}]"
>>>>>>                 return format_string.format(progress, collected)
>>>>>>             return f" [ {collected} / {collected} ]"
    5:         if self._show_progress_info == "times":
>>>>>>             if not collected:
>>>>>>                 return ""
>>>>>>             all_reports = (
>>>>>>                 self._get_reports_to_display("passed")
>>>>>>                 + self._get_reports_to_display("xpassed")
>>>>>>                 + self._get_reports_to_display("failed")
>>>>>>                 + self._get_reports_to_display("xfailed")
>>>>>>                 + self._get_reports_to_display("skipped")
>>>>>>                 + self._get_reports_to_display("error")
>>>>>>                 + self._get_reports_to_display("")
                   )
>>>>>>             current_location = all_reports[-1].location[0]
>>>>>>             not_reported = [
>>>>>>                 r for r in all_reports if r.nodeid not in self._timing_nodeids_reported
                   ]
>>>>>>             tests_in_module = sum(
>>>>>>                 i.location[0] == current_location for i in self._session.items
                   )
>>>>>>             tests_completed = sum(
>>>>>>                 r.when == "setup"
>>>>>>                 for r in not_reported
>>>>>>                 if r.location[0] == current_location
                   )
>>>>>>             last_in_module = tests_completed == tests_in_module
>>>>>>             if self.showlongtestinfo or last_in_module:
>>>>>>                 self._timing_nodeids_reported.update(r.nodeid for r in not_reported)
>>>>>>                 return format_node_duration(
>>>>>>                     sum(r.duration for r in not_reported if isinstance(r, TestReport))
                       )
>>>>>>             return ""
    5:         if collected:
    5:             return f" [{self.reported_progress * 100 // collected:3d}%]"
>>>>>>         return " [100%]"
       
>>>>>>     def _write_progress_information_if_past_edge(self) -> None:
   88:         w = self._width_of_current_line
   88:         if self._show_progress_info == "count":
>>>>>>             assert self._session
>>>>>>             num_tests = self._session.testscollected
>>>>>>             progress_length = len(f" [{num_tests}/{num_tests}]")
   88:         elif self._show_progress_info == "times":
>>>>>>             progress_length = len(" 99h 59m")
               else:
   88:             progress_length = len(" [100%]")
   88:         past_edge = w + progress_length + 1 >= self._screen_width
   88:         if past_edge:
>>>>>>             main_color, _ = self._get_main_color()
>>>>>>             msg = self._get_progress_information_message()
>>>>>>             self._tw.write(msg + "\n", **{main_color: True})
       
>>>>>>     def _write_progress_information_filling_space(self) -> None:
    5:         color, _ = self._get_main_color()
    5:         msg = self._get_progress_information_message()
    5:         w = self._width_of_current_line
    5:         fill = self._tw.fullwidth - w - 1
    5:         self.write(msg.rjust(fill), flush=True, **{color: True})
       
>>>>>>     @property
>>>>>>     def _width_of_current_line(self) -> int:
               """Return the width of the current line."""
   93:         return self._tw.width_of_current_line
       
>>>>>>     def pytest_collection(self) -> None:
    1:         if self.isatty():
>>>>>>             if self.config.option.verbose >= 0:
>>>>>>                 self.write("collecting ... ", flush=True, bold=True)
    1:         elif self.config.option.verbose >= 1:
>>>>>>             self.write("collecting ... ", flush=True, bold=True)
       
>>>>>>     def pytest_collectreport(self, report: CollectReport) -> None:
   35:         if report.failed:
>>>>>>             self._add_stats("error", [report])
   35:         elif report.skipped:
>>>>>>             self._add_stats("skipped", [report])
  158:         items = [x for x in report.result if isinstance(x, Item)]
   35:         self._numcollected += len(items)
   35:         if self.isatty():
>>>>>>             self.report_collect()
       
>>>>>>     def report_collect(self, final: bool = False) -> None:
    1:         if self.config.option.verbose < 0:
>>>>>>             return
       
    1:         if not final:
                   # Only write the "collecting" report every `REPORT_COLLECTING_RESOLUTION`.
                   if (
>>>>>>                 self._collect_report_last_write.elapsed().seconds
>>>>>>                 < REPORT_COLLECTING_RESOLUTION
                   ):
>>>>>>                 return
>>>>>>             self._collect_report_last_write = timing.Instant()
       
    1:         errors = len(self.stats.get("error", []))
    1:         skipped = len(self.stats.get("skipped", []))
    1:         deselected = len(self.stats.get("deselected", []))
    1:         selected = self._numcollected - deselected
    1:         line = "collected " if final else "collecting "
    2:         line += (
    1:             str(self._numcollected) + " item" + ("" if self._numcollected == 1 else "s")
               )
    1:         if errors:
>>>>>>             line += f" / {errors} error{'s' if errors != 1 else ''}"
    1:         if deselected:
>>>>>>             line += f" / {deselected} deselected"
    1:         if skipped:
>>>>>>             line += f" / {skipped} skipped"
    1:         if self._numcollected > selected:
>>>>>>             line += f" / {selected} selected"
    1:         if self.isatty():
>>>>>>             self.rewrite(line, bold=True, erase=True)
>>>>>>             if final:
>>>>>>                 self.write("\n")
               else:
    1:             self.write_line(line)
       
>>>>>>     @hookimpl(trylast=True)
>>>>>>     def pytest_sessionstart(self, session: Session) -> None:
    1:         self._session = session
    1:         self._session_start = timing.Instant()
    1:         if not self.showheader:
>>>>>>             return
    1:         self.write_sep("=", "test session starts", bold=True)
    1:         verinfo = platform.python_version()
    1:         if not self.no_header:
    1:             msg = f"platform {sys.platform} -- Python {verinfo}"
    1:             pypy_version_info = getattr(sys, "pypy_version_info", None)
    1:             if pypy_version_info:
>>>>>>                 verinfo = ".".join(map(str, pypy_version_info[:3]))
>>>>>>                 msg += f"[pypy-{verinfo}-{pypy_version_info[3]}]"
    1:             msg += f", pytest-{_pytest._version.version}, pluggy-{pluggy.__version__}"
                   if (
    1:                 self.verbosity > 0
    1:                 or self.config.option.debug
    1:                 or getattr(self.config.option, "pastebin", None)
                   ):
>>>>>>                 msg += " -- " + str(sys.executable)
    1:             self.write_line(msg)
    2:             lines = self.config.hook.pytest_report_header(
    1:                 config=self.config, start_path=self.startpath
                   )
    1:             self._write_report_lines_from_hooks(lines)
       
>>>>>>     def _write_report_lines_from_hooks(
>>>>>>         self, lines: Sequence[str | Sequence[str]]
>>>>>>     ) -> None:
    5:         for line_or_lines in reversed(lines):
    3:             if isinstance(line_or_lines, str):
>>>>>>                 self.write_line(line_or_lines)
                   else:
    8:                 for line in line_or_lines:
    5:                     self.write_line(line)
       
>>>>>>     def pytest_report_header(self, config: Config) -> list[str]:
    1:         result = [f"rootdir: {config.rootpath}"]
       
    1:         if config.inipath:
    1:             warning = ""
    1:             if config._ignored_config_files:
>>>>>>                 warning = f" (WARNING: ignoring pytest config in {', '.join(config._ignored_config_files)}!)"
    2:             result.append(
    1:                 "configfile: " + bestrelpath(config.rootpath, config.inipath) + warning
                   )
       
    1:         if config.args_source == Config.ArgsSource.TESTPATHS:
    1:             testpaths: list[str] = config.getini("testpaths")
    1:             result.append("testpaths: {}".format(", ".join(testpaths)))
       
    1:         plugininfo = config.pluginmanager.list_plugin_distinfo()
    1:         if plugininfo:
    2:             result.append(
    1:                 "plugins: {}".format(", ".join(_plugin_nameversions(plugininfo)))
                   )
    1:         return result
       
>>>>>>     def pytest_collection_finish(self, session: Session) -> None:
    1:         self.report_collect(True)
       
    2:         lines = self.config.hook.pytest_report_collectionfinish(
    1:             config=self.config,
    1:             start_path=self.startpath,
    1:             items=session.items,
               )
    1:         self._write_report_lines_from_hooks(lines)
       
    1:         if self.config.getoption("collectonly"):
>>>>>>             if session.items:
>>>>>>                 if self.config.option.verbose > -1:
>>>>>>                     self._tw.line("")
>>>>>>                 self._printcollecteditems(session.items)
       
>>>>>>             failed = self.stats.get("failed")
>>>>>>             if failed:
>>>>>>                 self._tw.sep("!", "collection failures")
>>>>>>                 for rep in failed:
>>>>>>                     rep.toterminal(self._tw)
       
>>>>>>     def _printcollecteditems(self, items: Sequence[Item]) -> None:
>>>>>>         test_cases_verbosity = self.config.get_verbosity(Config.VERBOSITY_TEST_CASES)
>>>>>>         if test_cases_verbosity < 0:
>>>>>>             if test_cases_verbosity < -1:
>>>>>>                 counts = Counter(item.nodeid.split("::", 1)[0] for item in items)
>>>>>>                 for name, count in sorted(counts.items()):
>>>>>>                     self._tw.line(f"{name}: {count}")
                   else:
>>>>>>                 for item in items:
>>>>>>                     self._tw.line(item.nodeid)
>>>>>>             return
>>>>>>         stack: list[Node] = []
>>>>>>         indent = ""
>>>>>>         for item in items:
>>>>>>             needed_collectors = item.listchain()[1:]  # strip root node
>>>>>>             while stack:
>>>>>>                 if stack == needed_collectors[: len(stack)]:
>>>>>>                     break
>>>>>>                 stack.pop()
>>>>>>             for col in needed_collectors[len(stack) :]:
>>>>>>                 stack.append(col)
>>>>>>                 indent = (len(stack) - 1) * "  "
>>>>>>                 self._tw.line(f"{indent}{col}")
>>>>>>                 if test_cases_verbosity >= 1:
>>>>>>                     obj = getattr(col, "obj", None)
>>>>>>                     doc = inspect.getdoc(obj) if obj else None
>>>>>>                     if doc:
>>>>>>                         for line in doc.splitlines():
>>>>>>                             self._tw.line("{}{}".format(indent + "  ", line))
       
>>>>>>     @hookimpl(wrapper=True)
>>>>>>     def pytest_sessionfinish(
>>>>>>         self, session: Session, exitstatus: int | ExitCode
>>>>>>     ) -> Generator[None]:
    1:         result = yield
    1:         self._tw.line("")
    1:         summary_exit_codes = (
    1:             ExitCode.OK,
    1:             ExitCode.TESTS_FAILED,
    1:             ExitCode.INTERRUPTED,
    1:             ExitCode.USAGE_ERROR,
    1:             ExitCode.NO_TESTS_COLLECTED,
               )
    1:         if exitstatus in summary_exit_codes and not self.no_summary:
    2:             self.config.hook.pytest_terminal_summary(
    1:                 terminalreporter=self, exitstatus=exitstatus, config=self.config
                   )
    1:         if session.shouldfail:
>>>>>>             self.write_sep("!", str(session.shouldfail), red=True)
    1:         if exitstatus == ExitCode.INTERRUPTED:
>>>>>>             self._report_keyboardinterrupt()
>>>>>>             self._keyboardinterrupt_memo = None
    1:         elif session.shouldstop:
>>>>>>             self.write_sep("!", str(session.shouldstop), red=True)
    1:         self.summary_stats()
    1:         return result
       
>>>>>>     @hookimpl(wrapper=True)
>>>>>>     def pytest_terminal_summary(self) -> Generator[None]:
    1:         self.summary_errors()
    1:         self.summary_failures()
    1:         self.summary_xfailures()
    1:         self.summary_warnings()
    1:         self.summary_passes()
    1:         self.summary_xpasses()
    1:         try:
    1:             return (yield)
               finally:
    1:             self.short_test_summary()
                   # Display any extra warnings from teardown here (if any).
    1:             self.summary_warnings()
       
>>>>>>     def pytest_keyboard_interrupt(self, excinfo: ExceptionInfo[BaseException]) -> None:
>>>>>>         self._keyboardinterrupt_memo = excinfo.getrepr(funcargs=True)
       
>>>>>>     def pytest_unconfigure(self) -> None:
    1:         if self._keyboardinterrupt_memo is not None:
>>>>>>             self._report_keyboardinterrupt()
       
>>>>>>     def _report_keyboardinterrupt(self) -> None:
>>>>>>         excrepr = self._keyboardinterrupt_memo
>>>>>>         assert excrepr is not None
>>>>>>         assert excrepr.reprcrash is not None
>>>>>>         msg = excrepr.reprcrash.message
>>>>>>         self.write_sep("!", msg)
>>>>>>         if "KeyboardInterrupt" in msg:
>>>>>>             if self.config.option.fulltrace:
>>>>>>                 excrepr.toterminal(self._tw)
                   else:
>>>>>>                 excrepr.reprcrash.toterminal(self._tw)
>>>>>>                 self._tw.line(
>>>>>>                     "(to show a full traceback on KeyboardInterrupt use --full-trace)",
>>>>>>                     yellow=True,
                       )
       
>>>>>>     def _locationline(
>>>>>>         self, nodeid: str, fspath: str, lineno: int | None, domain: str
>>>>>>     ) -> str:
>>>>>>         def mkrel(nodeid: str) -> str:
>>>>>>             line = self.config.cwd_relative_nodeid(nodeid)
>>>>>>             if domain and line.endswith(domain):
>>>>>>                 line = line[: -len(domain)]
>>>>>>                 values = domain.split("[")
>>>>>>                 values[0] = values[0].replace(".", "::")  # don't replace '.' in params
>>>>>>                 line += "[".join(values)
>>>>>>             return line
       
               # fspath comes from testid which has a "/"-normalized path.
>>>>>>         if fspath:
>>>>>>             res = mkrel(nodeid)
>>>>>>             if self.verbosity >= 2 and nodeid.split("::")[0] != fspath.replace(
>>>>>>                 "\\", nodes.SEP
                   ):
>>>>>>                 res += " <- " + bestrelpath(self.startpath, Path(fspath))
               else:
>>>>>>             res = "[location]"
>>>>>>         return res + " "
       
>>>>>>     def _getfailureheadline(self, rep):
>>>>>>         head_line = rep.head_line
>>>>>>         if head_line:
>>>>>>             return head_line
>>>>>>         return "test session"  # XXX?
       
>>>>>>     def _getcrashline(self, rep):
>>>>>>         try:
>>>>>>             return str(rep.longrepr.reprcrash)
>>>>>>         except AttributeError:
>>>>>>             try:
>>>>>>                 return str(rep.longrepr)[:50]
>>>>>>             except AttributeError:
>>>>>>                 return ""
       
           #
           # Summaries for sessionfinish.
           #
>>>>>>     def getreports(self, name: str):
    2:         return [x for x in self.stats.get(name, ()) if not hasattr(x, "_pdbshown")]
       
>>>>>>     def summary_warnings(self) -> None:
    2:         if self.hasopt("w"):
    2:             all_warnings: list[WarningReport] | None = self.stats.get("warnings")
    2:             if not all_warnings:
>>>>>>                 return
       
    2:             final = self._already_displayed_warnings is not None
    2:             if final:
    1:                 warning_reports = all_warnings[self._already_displayed_warnings :]
                   else:
    1:                 warning_reports = all_warnings
    2:             self._already_displayed_warnings = len(warning_reports)
    2:             if not warning_reports:
    1:                 return
       
    1:             reports_grouped_by_message: dict[str, list[WarningReport]] = {}
    8:             for wr in warning_reports:
    7:                 reports_grouped_by_message.setdefault(wr.message, []).append(wr)
       
    1:             def collapsed_location_report(reports: list[WarningReport]) -> str:
    5:                 locations = []
   12:                 for w in reports:
    7:                     location = w.get_location(self.config)
    7:                     if location:
    7:                         locations.append(location)
       
    5:                 if len(locations) < 10:
    5:                     return "\n".join(map(str, locations))
       
>>>>>>                 counts_by_filename = Counter(
>>>>>>                     str(loc).split("::", 1)[0] for loc in locations
                       )
>>>>>>                 return "\n".join(
>>>>>>                     "{}: {} warning{}".format(k, v, "s" if v > 1 else "")
>>>>>>                     for k, v in counts_by_filename.items()
                       )
       
    1:             title = "warnings summary (final)" if final else "warnings summary"
    1:             self.write_sep("=", title, yellow=True, bold=False)
    6:             for message, message_reports in reports_grouped_by_message.items():
    5:                 maybe_location = collapsed_location_report(message_reports)
    5:                 if maybe_location:
    5:                     self._tw.line(maybe_location)
    5:                     lines = message.splitlines()
   24:                     indented = "\n".join("  " + x for x in lines)
    5:                     message = indented.rstrip()
                       else:
>>>>>>                     message = message.rstrip()
    5:                 self._tw.line(message)
    5:                 self._tw.line()
    2:             self._tw.line(
    1:                 "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html"
                   )
       
>>>>>>     def summary_passes(self) -> None:
    1:         self.summary_passes_combined("passed", "PASSES", "P")
       
>>>>>>     def summary_xpasses(self) -> None:
    1:         self.summary_passes_combined("xpassed", "XPASSES", "X")
       
>>>>>>     def summary_passes_combined(
>>>>>>         self, which_reports: str, sep_title: str, needed_opt: str
>>>>>>     ) -> None:
    2:         if self.config.option.tbstyle != "no":
    2:             if self.hasopt(needed_opt):
>>>>>>                 reports: list[TestReport] = self.getreports(which_reports)
>>>>>>                 if not reports:
>>>>>>                     return
>>>>>>                 self.write_sep("=", sep_title)
>>>>>>                 for rep in reports:
>>>>>>                     if rep.sections:
>>>>>>                         msg = self._getfailureheadline(rep)
>>>>>>                         self.write_sep("_", msg, green=True, bold=True)
>>>>>>                         self._outrep_summary(rep)
>>>>>>                     self._handle_teardown_sections(rep.nodeid)
       
>>>>>>     def _get_teardown_reports(self, nodeid: str) -> list[TestReport]:
>>>>>>         reports = self.getreports("")
>>>>>>         return [
>>>>>>             report
>>>>>>             for report in reports
>>>>>>             if report.when == "teardown" and report.nodeid == nodeid
               ]
       
>>>>>>     def _handle_teardown_sections(self, nodeid: str) -> None:
>>>>>>         for report in self._get_teardown_reports(nodeid):
>>>>>>             self.print_teardown_sections(report)
       
>>>>>>     def print_teardown_sections(self, rep: TestReport) -> None:
>>>>>>         showcapture = self.config.option.showcapture
>>>>>>         if showcapture == "no":
>>>>>>             return
>>>>>>         for secname, content in rep.sections:
>>>>>>             if showcapture != "all" and showcapture not in secname:
>>>>>>                 continue
>>>>>>             if "teardown" in secname:
>>>>>>                 self._tw.sep("-", secname)
>>>>>>                 if content[-1:] == "\n":
>>>>>>                     content = content[:-1]
>>>>>>                 self._tw.line(content)
       
>>>>>>     def summary_failures(self) -> None:
    1:         style = self.config.option.tbstyle
    1:         self.summary_failures_combined("failed", "FAILURES", style=style)
       
>>>>>>     def summary_xfailures(self) -> None:
    1:         show_tb = self.config.option.xfail_tb
    1:         style = self.config.option.tbstyle if show_tb else "no"
    1:         self.summary_failures_combined("xfailed", "XFAILURES", style=style)
       
>>>>>>     def summary_failures_combined(
               self,
>>>>>>         which_reports: str,
>>>>>>         sep_title: str,
               *,
>>>>>>         style: str,
>>>>>>         needed_opt: str | None = None,
>>>>>>     ) -> None:
    2:         if style != "no":
    1:             if not needed_opt or self.hasopt(needed_opt):
    1:                 reports: list[BaseReport] = self.getreports(which_reports)
    1:                 if not reports:
    1:                     return
>>>>>>                 self.write_sep("=", sep_title)
>>>>>>                 if style == "line":
>>>>>>                     for rep in reports:
>>>>>>                         line = self._getcrashline(rep)
>>>>>>                         self._outrep_summary(rep)
>>>>>>                         self.write_line(line)
                       else:
>>>>>>                     for rep in reports:
>>>>>>                         msg = self._getfailureheadline(rep)
>>>>>>                         self.write_sep("_", msg, red=True, bold=True)
>>>>>>                         self._outrep_summary(rep)
>>>>>>                         self._handle_teardown_sections(rep.nodeid)
       
>>>>>>     def summary_errors(self) -> None:
    1:         if self.config.option.tbstyle != "no":
    1:             reports: list[BaseReport] = self.getreports("error")
    1:             if not reports:
    1:                 return
>>>>>>             self.write_sep("=", "ERRORS")
>>>>>>             for rep in self.stats["error"]:
>>>>>>                 msg = self._getfailureheadline(rep)
>>>>>>                 if rep.when == "collect":
>>>>>>                     msg = "ERROR collecting " + msg
                       else:
>>>>>>                     msg = f"ERROR at {rep.when} of {msg}"
>>>>>>                 self.write_sep("_", msg, red=True, bold=True)
>>>>>>                 self._outrep_summary(rep)
       
>>>>>>     def _outrep_summary(self, rep: BaseReport) -> None:
>>>>>>         rep.toterminal(self._tw)
>>>>>>         showcapture = self.config.option.showcapture
>>>>>>         if showcapture == "no":
>>>>>>             return
>>>>>>         for secname, content in rep.sections:
>>>>>>             if showcapture != "all" and showcapture not in secname:
>>>>>>                 continue
>>>>>>             self._tw.sep("-", secname)
>>>>>>             if content[-1:] == "\n":
>>>>>>                 content = content[:-1]
>>>>>>             self._tw.line(content)
       
>>>>>>     def summary_stats(self) -> None:
    1:         if self.verbosity < -1:
>>>>>>             return
       
    1:         session_duration = self._session_start.elapsed()
    1:         (parts, main_color) = self.build_summary_stats_line()
    1:         line_parts = []
       
    1:         display_sep = self.verbosity >= 0
    1:         if display_sep:
    1:             fullwidth = self._tw.fullwidth
    3:         for text, markup in parts:
    2:             with_markup = self._tw.markup(text, **markup)
    2:             if display_sep:
    2:                 fullwidth += len(with_markup) - len(text)
    2:             line_parts.append(with_markup)
    1:         msg = ", ".join(line_parts)
       
    1:         main_markup = {main_color: True}
    1:         duration = f" in {format_session_duration(session_duration.seconds)}"
    1:         duration_with_markup = self._tw.markup(duration, **main_markup)
    1:         if display_sep:
    1:             fullwidth += len(duration_with_markup) - len(duration)
    1:         msg += duration_with_markup
       
    1:         if display_sep:
    1:             markup_for_end_sep = self._tw.markup("", **main_markup)
    1:             if markup_for_end_sep.endswith("\x1b[0m"):
>>>>>>                 markup_for_end_sep = markup_for_end_sep[:-4]
    1:             fullwidth += len(markup_for_end_sep)
    1:             msg += markup_for_end_sep
       
    1:         if display_sep:
    1:             self.write_sep("=", msg, fullwidth=fullwidth, **main_markup)
               else:
>>>>>>             self.write_line(msg, **main_markup)
       
>>>>>>     def short_test_summary(self) -> None:
    1:         if not self.reportchars:
>>>>>>             return
       
    1:         def show_simple(lines: list[str], *, stat: str) -> None:
    2:             failed = self.stats.get(stat, [])
    2:             if not failed:
    2:                 return
>>>>>>             config = self.config
>>>>>>             for rep in failed:
>>>>>>                 color = _color_for_type.get(stat, _color_for_type_default)
>>>>>>                 line = _get_line_with_reprcrash_message(
>>>>>>                     config, rep, self._tw, {color: True}
                       )
>>>>>>                 lines.append(line)
       
    1:         def show_xfailed(lines: list[str]) -> None:
>>>>>>             xfailed = self.stats.get("xfailed", [])
>>>>>>             for rep in xfailed:
>>>>>>                 verbose_word, verbose_markup = rep._get_verbose_word_with_markup(
>>>>>>                     self.config, {_color_for_type["warnings"]: True}
                       )
>>>>>>                 markup_word = self._tw.markup(verbose_word, **verbose_markup)
>>>>>>                 nodeid = _get_node_id_with_markup(self._tw, self.config, rep)
>>>>>>                 line = f"{markup_word} {nodeid}"
>>>>>>                 reason = rep.wasxfail
>>>>>>                 if reason:
>>>>>>                     line += " - " + str(reason)
       
>>>>>>                 lines.append(line)
       
    1:         def show_xpassed(lines: list[str]) -> None:
>>>>>>             xpassed = self.stats.get("xpassed", [])
>>>>>>             for rep in xpassed:
>>>>>>                 verbose_word, verbose_markup = rep._get_verbose_word_with_markup(
>>>>>>                     self.config, {_color_for_type["warnings"]: True}
                       )
>>>>>>                 markup_word = self._tw.markup(verbose_word, **verbose_markup)
>>>>>>                 nodeid = _get_node_id_with_markup(self._tw, self.config, rep)
>>>>>>                 line = f"{markup_word} {nodeid}"
>>>>>>                 reason = rep.wasxfail
>>>>>>                 if reason:
>>>>>>                     line += " - " + str(reason)
>>>>>>                 lines.append(line)
       
    1:         def show_skipped_folded(lines: list[str]) -> None:
>>>>>>             skipped: list[CollectReport] = self.stats.get("skipped", [])
>>>>>>             fskips = _folded_skips(self.startpath, skipped) if skipped else []
>>>>>>             if not fskips:
>>>>>>                 return
>>>>>>             verbose_word, verbose_markup = skipped[0]._get_verbose_word_with_markup(
>>>>>>                 self.config, {_color_for_type["warnings"]: True}
                   )
>>>>>>             markup_word = self._tw.markup(verbose_word, **verbose_markup)
>>>>>>             prefix = "Skipped: "
>>>>>>             for num, fspath, lineno, reason in fskips:
>>>>>>                 if reason.startswith(prefix):
>>>>>>                     reason = reason[len(prefix) :]
>>>>>>                 if lineno is not None:
>>>>>>                     lines.append(f"{markup_word} [{num}] {fspath}:{lineno}: {reason}")
                       else:
>>>>>>                     lines.append(f"{markup_word} [{num}] {fspath}: {reason}")
       
    1:         def show_skipped_unfolded(lines: list[str]) -> None:
>>>>>>             skipped: list[CollectReport] = self.stats.get("skipped", [])
       
>>>>>>             for rep in skipped:
>>>>>>                 assert rep.longrepr is not None
>>>>>>                 assert isinstance(rep.longrepr, tuple), (rep, rep.longrepr)
>>>>>>                 assert len(rep.longrepr) == 3, (rep, rep.longrepr)
       
>>>>>>                 verbose_word, verbose_markup = rep._get_verbose_word_with_markup(
>>>>>>                     self.config, {_color_for_type["warnings"]: True}
                       )
>>>>>>                 markup_word = self._tw.markup(verbose_word, **verbose_markup)
>>>>>>                 nodeid = _get_node_id_with_markup(self._tw, self.config, rep)
>>>>>>                 line = f"{markup_word} {nodeid}"
>>>>>>                 reason = rep.longrepr[2]
>>>>>>                 if reason:
>>>>>>                     line += " - " + str(reason)
>>>>>>                 lines.append(line)
       
    1:         def show_skipped(lines: list[str]) -> None:
>>>>>>             if self.foldskipped:
>>>>>>                 show_skipped_folded(lines)
                   else:
>>>>>>                 show_skipped_unfolded(lines)
       
    1:         REPORTCHAR_ACTIONS: Mapping[str, Callable[[list[str]], None]] = {
    1:             "x": show_xfailed,
    1:             "X": show_xpassed,
    1:             "f": partial(show_simple, stat="failed"),
    1:             "s": show_skipped,
    1:             "p": partial(show_simple, stat="passed"),
    1:             "E": partial(show_simple, stat="error"),
               }
       
    1:         lines: list[str] = []
    4:         for char in self.reportchars:
    3:             action = REPORTCHAR_ACTIONS.get(char)
    3:             if action:  # skipping e.g. "P" (passed with output) here.
    2:                 action(lines)
       
    1:         if lines:
>>>>>>             self.write_sep("=", "short test summary info", cyan=True, bold=True)
>>>>>>             for line in lines:
>>>>>>                 self.write_line(line)
       
>>>>>>     def _get_main_color(self) -> tuple[str, list[str]]:
    6:         if self._main_color is None or self._known_types is None or self._is_last_item:
    2:             self._set_main_color()
    2:             assert self._main_color
    2:             assert self._known_types
    6:         return self._main_color, self._known_types
       
>>>>>>     def _determine_main_color(self, unknown_type_seen: bool) -> str:
    5:         stats = self.stats
    5:         if "failed" in stats or "error" in stats:
>>>>>>             main_color = "red"
    5:         elif "warnings" in stats or "xpassed" in stats or unknown_type_seen:
    5:             main_color = "yellow"
>>>>>>         elif "passed" in stats or not self._is_last_item:
>>>>>>             main_color = "green"
               else:
>>>>>>             main_color = "yellow"
    5:         return main_color
       
>>>>>>     def _set_main_color(self) -> None:
    5:         unknown_types: list[str] = []
   17:         for found_type in self.stats:
   12:             if found_type:  # setup/teardown reports have an empty key, ignore them
    8:                 if found_type not in KNOWN_TYPES and found_type not in unknown_types:
>>>>>>                     unknown_types.append(found_type)
    5:         self._known_types = list(KNOWN_TYPES) + unknown_types
    5:         self._main_color = self._determine_main_color(bool(unknown_types))
       
>>>>>>     def build_summary_stats_line(self) -> tuple[list[tuple[str, dict[str, bool]]], str]:
               """
               Build the parts used in the last summary stats line.
       
               The summary stats line is the line shown at the end, "=== 12 passed, 2 errors in Xs===".
       
               This function builds a list of the "parts" that make up for the text in that line, in
               the example above it would be::
       
                   [
                       ("12 passed", {"green": True}),
                       ("2 errors", {"red": True}
                   ]
       
               That last dict for each line is a "markup dictionary", used by TerminalWriter to
               color output.
       
               The final color of the line is also determined by this function, and is the second
               element of the returned tuple.
               """
    1:         if self.config.getoption("collectonly"):
>>>>>>             return self._build_collect_only_summary_stats_line()
               else:
    1:             return self._build_normal_summary_stats_line()
       
>>>>>>     def _get_reports_to_display(self, key: str) -> list[Any]:
               """Get test/collection reports for the given status key, such as `passed` or `error`."""
   11:         reports = self.stats.get(key, [])
  107:         return [x for x in reports if getattr(x, "count_towards_summary", True)]
       
>>>>>>     def _build_normal_summary_stats_line(
               self,
>>>>>>     ) -> tuple[list[tuple[str, dict[str, bool]]], str]:
    1:         main_color, known_types = self._get_main_color()
    1:         parts = []
       
   12:         for key in known_types:
   11:             reports = self._get_reports_to_display(key)
   11:             if reports:
    2:                 count = len(reports)
    2:                 color = _color_for_type.get(key, _color_for_type_default)
    2:                 markup = {color: True, "bold": color == main_color}
    2:                 parts.append(("%d %s" % pluralize(count, key), markup))  # noqa: UP031
       
    1:         if not parts:
>>>>>>             parts = [("no tests ran", {_color_for_type_default: True})]
       
    1:         return parts, main_color
       
>>>>>>     def _build_collect_only_summary_stats_line(
               self,
>>>>>>     ) -> tuple[list[tuple[str, dict[str, bool]]], str]:
>>>>>>         deselected = len(self._get_reports_to_display("deselected"))
>>>>>>         errors = len(self._get_reports_to_display("error"))
       
>>>>>>         if self._numcollected == 0:
>>>>>>             parts = [("no tests collected", {"yellow": True})]
>>>>>>             main_color = "yellow"
       
>>>>>>         elif deselected == 0:
>>>>>>             main_color = "green"
>>>>>>             collected_output = "%d %s collected" % pluralize(self._numcollected, "test")  # noqa: UP031
>>>>>>             parts = [(collected_output, {main_color: True})]
               else:
>>>>>>             all_tests_were_deselected = self._numcollected == deselected
>>>>>>             if all_tests_were_deselected:
>>>>>>                 main_color = "yellow"
>>>>>>                 collected_output = f"no tests collected ({deselected} deselected)"
                   else:
>>>>>>                 main_color = "green"
>>>>>>                 selected = self._numcollected - deselected
>>>>>>                 collected_output = f"{selected}/{self._numcollected} tests collected ({deselected} deselected)"
       
>>>>>>             parts = [(collected_output, {main_color: True})]
       
>>>>>>         if errors:
>>>>>>             main_color = _color_for_type["error"]
>>>>>>             parts += [("%d %s" % pluralize(errors, "error"), {main_color: True})]  # noqa: UP031
       
>>>>>>         return parts, main_color
       
       
>>>>>> def _get_node_id_with_markup(tw: TerminalWriter, config: Config, rep: BaseReport):
>>>>>>     nodeid = config.cwd_relative_nodeid(rep.nodeid)
>>>>>>     path, *parts = nodeid.split("::")
>>>>>>     if parts:
>>>>>>         parts_markup = tw.markup("::".join(parts), bold=True)
>>>>>>         return path + "::" + parts_markup
           else:
>>>>>>         return path
       
       
>>>>>> def _format_trimmed(format: str, msg: str, available_width: int) -> str | None:
           """Format msg into format, ellipsizing it if doesn't fit in available_width.
       
           Returns None if even the ellipsis can't fit.
           """
           # Only use the first line.
>>>>>>     i = msg.find("\n")
>>>>>>     if i != -1:
>>>>>>         msg = msg[:i]
       
>>>>>>     ellipsis = "..."
>>>>>>     format_width = wcswidth(format.format(""))
>>>>>>     if format_width + len(ellipsis) > available_width:
>>>>>>         return None
       
>>>>>>     if format_width + wcswidth(msg) > available_width:
>>>>>>         available_width -= len(ellipsis)
>>>>>>         msg = msg[:available_width]
>>>>>>         while format_width + wcswidth(msg) > available_width:
>>>>>>             msg = msg[:-1]
>>>>>>         msg += ellipsis
       
>>>>>>     return format.format(msg)
       
       
>>>>>> def _get_line_with_reprcrash_message(
>>>>>>     config: Config, rep: BaseReport, tw: TerminalWriter, word_markup: dict[str, bool]
>>>>>> ) -> str:
           """Get summary line for a report, trying to add reprcrash message."""
>>>>>>     verbose_word, verbose_markup = rep._get_verbose_word_with_markup(
>>>>>>         config, word_markup
           )
>>>>>>     word = tw.markup(verbose_word, **verbose_markup)
>>>>>>     node = _get_node_id_with_markup(tw, config, rep)
       
>>>>>>     line = f"{word} {node}"
>>>>>>     line_width = wcswidth(line)
       
           msg: str | None
>>>>>>     try:
>>>>>>         if isinstance(rep.longrepr, str):
>>>>>>             msg = rep.longrepr
               else:
                   # Type ignored intentionally -- possible AttributeError expected.
>>>>>>             msg = rep.longrepr.reprcrash.message  # type: ignore[union-attr]
>>>>>>     except AttributeError:
>>>>>>         pass
           else:
               if (
>>>>>>             running_on_ci() or config.option.verbose >= 2
>>>>>>         ) and not config.option.force_short_summary:
>>>>>>             msg = f" - {msg}"
               else:
>>>>>>             available_width = tw.fullwidth - line_width
>>>>>>             msg = _format_trimmed(" - {}", msg, available_width)
>>>>>>         if msg is not None:
>>>>>>             line += msg
       
>>>>>>     return line
       
       
>>>>>> def _folded_skips(
>>>>>>     startpath: Path,
>>>>>>     skipped: Sequence[CollectReport],
>>>>>> ) -> list[tuple[int, str, int | None, str]]:
>>>>>>     d: dict[tuple[str, int | None, str], list[CollectReport]] = {}
>>>>>>     for event in skipped:
>>>>>>         assert event.longrepr is not None
>>>>>>         assert isinstance(event.longrepr, tuple), (event, event.longrepr)
>>>>>>         assert len(event.longrepr) == 3, (event, event.longrepr)
>>>>>>         fspath, lineno, reason = event.longrepr
               # For consistency, report all fspaths in relative form.
>>>>>>         fspath = bestrelpath(startpath, Path(fspath))
>>>>>>         keywords = getattr(event, "keywords", {})
               # Folding reports with global pytestmark variable.
               # This is a workaround, because for now we cannot identify the scope of a skip marker
               # TODO: Revisit after marks scope would be fixed.
               if (
>>>>>>             event.when == "setup"
>>>>>>             and "skip" in keywords
>>>>>>             and "pytestmark" not in keywords
               ):
>>>>>>             key: tuple[str, int | None, str] = (fspath, None, reason)
               else:
>>>>>>             key = (fspath, lineno, reason)
>>>>>>         d.setdefault(key, []).append(event)
>>>>>>     values: list[tuple[int, str, int | None, str]] = []
>>>>>>     for key, events in d.items():
>>>>>>         values.append((len(events), *key))
>>>>>>     return values
       
       
>>>>>> _color_for_type = {
>>>>>>     "failed": "red",
>>>>>>     "error": "red",
>>>>>>     "warnings": "yellow",
>>>>>>     "passed": "green",
>>>>>>     "subtests passed": "green",
>>>>>>     "subtests failed": "red",
       }
>>>>>> _color_for_type_default = "yellow"
       
       
>>>>>> def pluralize(count: int, noun: str) -> tuple[int, str]:
           # No need to pluralize words such as `failed` or `passed`.
    2:     if noun not in ["error", "warnings", "test"]:
    1:         return count, noun
       
           # The `warnings` key is plural. To avoid API breakage, we keep it that way but
           # set it to singular here so we can determine plurality in the same way as we do
           # for `error`.
    1:     noun = noun.replace("warnings", "warning")
       
    1:     return count, noun + "s" if count != 1 else noun
       
       
>>>>>> def _plugin_nameversions(plugininfo) -> list[str]:
    1:     values: list[str] = []
    3:     for plugin, dist in plugininfo:
               # Gets us name and version!
    2:         name = f"{dist.project_name}-{dist.version}"
               # Questionable convenience, but it keeps things short.
    2:         if name.startswith("pytest-"):
    1:             name = name[7:]
               # We decided to print python package names they can have more than one plugin.
    2:         if name not in values:
    2:             values.append(name)
    1:     return values
       
       
>>>>>> def format_session_duration(seconds: float) -> str:
           """Format the given seconds in a human readable manner to show in the final summary."""
    1:     if seconds < 60:
    1:         return f"{seconds:.2f}s"
           else:
>>>>>>         dt = datetime.timedelta(seconds=int(seconds))
>>>>>>         return f"{seconds:.2f}s ({dt})"
       
       
>>>>>> def format_node_duration(seconds: float) -> str:
           """Format the given seconds in a human readable manner to show in the test progress."""
           # The formatting is designed to be compact and readable, with at most 7 characters
           # for durations below 100 hours.
>>>>>>     if seconds < 0.00001:
>>>>>>         return f" {seconds * 1000000:.3f}us"
>>>>>>     if seconds < 0.0001:
>>>>>>         return f" {seconds * 1000000:.2f}us"
>>>>>>     if seconds < 0.001:
>>>>>>         return f" {seconds * 1000000:.1f}us"
>>>>>>     if seconds < 0.01:
>>>>>>         return f" {seconds * 1000:.3f}ms"
>>>>>>     if seconds < 0.1:
>>>>>>         return f" {seconds * 1000:.2f}ms"
>>>>>>     if seconds < 1:
>>>>>>         return f" {seconds * 1000:.1f}ms"
>>>>>>     if seconds < 60:
>>>>>>         return f" {seconds:.3f}s"
>>>>>>     if seconds < 3600:
>>>>>>         return f" {seconds // 60:.0f}m {seconds % 60:.0f}s"
>>>>>>     return f" {seconds // 3600:.0f}h {(seconds % 3600) // 60:.0f}m"
       
       
>>>>>> def _get_raw_skip_reason(report: TestReport) -> str:
           """Get the reason string of a skip/xfail/xpass test report.
       
           The string is just the part given by the user.
           """
>>>>>>     if hasattr(report, "wasxfail"):
>>>>>>         reason = report.wasxfail
>>>>>>         if reason.startswith("reason: "):
>>>>>>             reason = reason[len("reason: ") :]
>>>>>>         return reason
           else:
>>>>>>         assert report.skipped
>>>>>>         assert isinstance(report.longrepr, tuple)
>>>>>>         _, _, reason = report.longrepr
>>>>>>         if reason.startswith("Skipped: "):
>>>>>>             reason = reason[len("Skipped: ") :]
>>>>>>         elif reason == "Skipped":
>>>>>>             reason = ""
>>>>>>         return reason
       
       
>>>>>> class TerminalProgressPlugin:
           """Terminal progress reporting plugin using OSC 9;4 ANSI sequences.
       
           Emits OSC 9;4 sequences to indicate test progress to terminal
           tabs/windows/etc.
       
           Not all terminal emulators support this feature.
       
           Ref: https://conemu.github.io/en/AnsiEscapeCodes.html#ConEmu_specific_OSC
           """
       
>>>>>>     def __init__(self, tr: TerminalReporter) -> None:
>>>>>>         self._tr = tr
>>>>>>         self._session: Session | None = None
>>>>>>         self._has_failures = False
       
>>>>>>     def _emit_progress(
               self,
>>>>>>         state: Literal["remove", "normal", "error", "indeterminate", "paused"],
>>>>>>         progress: int | None = None,
>>>>>>     ) -> None:
               """Emit OSC 9;4 sequence for indicating progress to the terminal.
       
               :param state:
                   Progress state to set.
               :param progress:
                   Progress value 0-100. Required for "normal", optional for "error"
                   and "paused", otherwise ignored.
               """
>>>>>>         assert progress is None or 0 <= progress <= 100
       
               # OSC 9;4 sequence: ESC ] 9 ; 4 ; state ; progress ST
               # ST can be ESC \ or BEL. ESC \ seems better supported.
>>>>>>         match state:
>>>>>>             case "remove":
>>>>>>                 sequence = "\x1b]9;4;0;\x1b\\"
>>>>>>             case "normal":
>>>>>>                 assert progress is not None
>>>>>>                 sequence = f"\x1b]9;4;1;{progress}\x1b\\"
>>>>>>             case "error":
>>>>>>                 if progress is not None:
>>>>>>                     sequence = f"\x1b]9;4;2;{progress}\x1b\\"
                       else:
>>>>>>                     sequence = "\x1b]9;4;2;\x1b\\"
>>>>>>             case "indeterminate":
>>>>>>                 sequence = "\x1b]9;4;3;\x1b\\"
>>>>>>             case "paused":
>>>>>>                 if progress is not None:
>>>>>>                     sequence = f"\x1b]9;4;4;{progress}\x1b\\"
                       else:
>>>>>>                     sequence = "\x1b]9;4;4;\x1b\\"
       
>>>>>>         self._tr.write_raw(sequence, flush=True)
       
>>>>>>     @hookimpl
>>>>>>     def pytest_sessionstart(self, session: Session) -> None:
>>>>>>         self._session = session
               # Show indeterminate progress during collection.
>>>>>>         self._emit_progress("indeterminate")
       
>>>>>>     @hookimpl
>>>>>>     def pytest_collection_finish(self) -> None:
>>>>>>         assert self._session is not None
>>>>>>         if self._session.testscollected > 0:
                   # Switch from indeterminate to 0% progress.
>>>>>>             self._emit_progress("normal", 0)
       
>>>>>>     @hookimpl
>>>>>>     def pytest_runtest_logreport(self, report: TestReport) -> None:
>>>>>>         if report.failed:
>>>>>>             self._has_failures = True
       
               # Let's consider the "call" phase for progress.
>>>>>>         if report.when != "call":
>>>>>>             return
       
               # Calculate and emit progress.
>>>>>>         assert self._session is not None
>>>>>>         collected = self._session.testscollected
>>>>>>         if collected > 0:
>>>>>>             reported = self._tr.reported_progress
>>>>>>             progress = min(reported * 100 // collected, 100)
>>>>>>             self._emit_progress("error" if self._has_failures else "normal", progress)
       
>>>>>>     @hookimpl
>>>>>>     def pytest_sessionfinish(self) -> None:
>>>>>>         self._emit_progress("remove")
